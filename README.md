# ğŸ©º DiaBuddy: A Diabetes Health Care Assistant using LLMs (LLaMA-3 + LoRA + RAG)

**Team 31**
ğŸ§‘â€ğŸ’» Vaibhav Vinay Ranashoor
ğŸ§‘â€ğŸ’» Rohan Jain
_CSE 574 â€“ (Spring 2025)_
University at Buffalo

---

## ğŸ”¬ Project Overview

DiaBuddy is a **privacy-preserving, LLM-based diabetes assistant** built with:

- ğŸ¦™ **LLaMA-3 8B Instruct model**
- ğŸ§  **LoRA-based fine-tuning** on 4K+ curated QA pairs
- ğŸ“š **RAG (Retrieval-Augmented Generation)** using a 2032-page trusted medical PDF
- ğŸ“Š Full evaluation using BLEU, ROUGE-L, BERTScore, Hallucination Rate, Cosine Similarity, and t-SNE

Our goal was to **enable lightweight, clinically coherent diabetic care chatbots** that run on a single GPU (Colab Pro, A100) and respect data localityâ€”**no cloud APIs**.

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ diabetes_qa_llama2_all_till_date_no_dup.jsonl   # Final curated SFT dataset
â”‚   â”œâ”€â”€ diabetes_qa_test_data.jsonl                     # 50-QA test set
â”‚   â””â”€â”€ small_rag_doc.pdf
â”‚   â””â”€â”€ small_rag_doc.pdf                               # 2032-page RAG data source
|
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ CSE_574_Project_Final_Report_Team_31.docx
â”‚   â””â”€â”€ Final_presentation_team_31.pptx
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Diabetes_LLM_Finetune_llama_3.ipynb             # LoRA fine-tuning notebook
â”‚   â””â”€â”€ Diabetes_Lllama3_finetune_and_RAG.ipynb         # RAG + Evaluation notebook
â”‚
â””â”€â”€ README.md                                           # â† You're here!
```

---

## ğŸ”§ Setup Instructions (Google Colab Only)

> ğŸ’» You need a **Google Colab Pro** subscription with **NVIDIA A100 GPU** access.

### âœ… 1. Mount Google Drive

```python
from google.colab import drive
drive.mount('/content/drive')
```

### ğŸ“¦ 2. Install Dependencies

```bash
!pip install --upgrade pip
!pip install transformers peft accelerate langchain faiss-cpu bitsandbytes \
sentence-transformers datasets evaluate bert_score
```

---

## ğŸ§  Part 1: Fine-Tuning LLaMA-3 using LoRA (SFT)

### ğŸ§¾ LoRA Configuration

- Base model: `meta-llama/Meta-Llama-3-8B-Instruct`
- LoRA: `r=8`, `alpha=16`, `dropout=0.1`
- Optimizer: AdamW, lr=2e-5
- Epochs: 3
- Trained on 4277 diabetes-specific Q\&A pairs

### ğŸ“ Fine-tuned Artifacts (Saved Separately in Google Drive)

You can download the pretrained LoRA adapter weights and tokenizer files here:
ğŸ”— [Google Drive â€“ LoRA Adapter Weights](https://drive.google.com/drive/folders/1bF4VCPMRd-TgX9WRt3Z1FS_rRCuYi0Ud?usp=sharing)

Contents:

```
adapter_model.safetensors
adapter_config.json
tokenizer.json
tokenizer_config.json
special_tokens_map.json
README.md
```

To use them:

```python
from peft import PeftModel, PeftConfig
config = PeftConfig.from_pretrained("<downloaded-folder-path>")
model = PeftModel.from_pretrained(base_model, "<downloaded-folder-path>")
```

---

## ğŸ’¡ To Reproduce This Project

> Clone this repo and run notebooks in Colab.

### ğŸ” Step-by-Step

1. Upload contents of `data/` to your Google Drive
2. Download LoRA adapter weights from this link:
   ğŸ”— [https://drive.google.com/drive/folders/1bF4VCPMRd-TgX9WRt3Z1FS_rRCuYi0Ud?usp=sharing](https://drive.google.com/drive/folders/1bF4VCPMRd-TgX9WRt3Z1FS_rRCuYi0Ud?usp=sharing)
   and place them in your Drive folder
3. Open `Diabetes_LLM_Finetune_llama_3.ipynb` â†’ Fine-tune (optional if using above)
4. Open `Diabetes_Lllama3_finetune_and_RAG.ipynb` â†’ Run:

   - PDF â†’ Chunking + FAISS
   - Generate responses (RAG + Non-RAG)
   - Evaluate BLEU, ROUGE-L, BERTScore, Cosine Sim, t-SNE, Hallucination

---

## ğŸ” Part 2: Retrieval-Augmented Generation (RAG)

### ğŸ“˜ Data Source

- `small_rag_doc.pdf` (2032 pages): Compiled official diabetes guidelines
- Chunked into 500-token segments using LangChain
- Embeddings via `all-MiniLM-L6-v2`
- Indexed using `FAISS`

### ğŸ”„ RAG Pipeline

```python
# Retrieve top-K context chunks
context = retriever.similarity_search(query, k=20)

# Concatenate context + user query â†’ generate response
response = model.generate(context + query)
```

---

## ğŸ“ˆ Evaluation Pipeline

| Metric            | Library              | Base Model           | Fine-Tuned + RAG                   |
| ----------------- | -------------------- | -------------------- | ---------------------------------- |
| **BLEU**          | ğŸ¤— evaluate          | âœ… 0.49%             | âœ… 0.81%                           |
| **ROUGE-L**       | ğŸ¤— evaluate          | âœ… 6.86%             | âœ… 8.23%                           |
| **BERTScore**     | ğŸ¤— bert-score        | âœ… 0.834             | âœ… 0.841                           |
| **Cosine Sim.**   | Sentence-Transformer | âœ… 0.474             | âœ… 0.448 (â†“ due to length/fluency) |
| **Hallucination** | BART-MNLI            | âœ… 0% contradictions | âœ… 0%, but more "Neutral"          |
| **t-SNE**         | sklearn              | ğŸ“‰ Overlapping       | âœ… Category clusters               |

### ğŸ“Š Hallucination Evaluation (BART-MNLI)

| Model | Entailment | Neutral | Contradiction (Hallucination) |
| ----- | ---------- | ------- | ----------------------------- |
| Base  | 46         | 4       | **0**                         |
| RAG   | 41         | 9       | **0**                         |

### ğŸ§  Key Insights

- RAG improves factuality but increases verbosity â†’ more neutrals
- No contradictions = No dangerous hallucinations ğŸš«
- Curated datasets + strong prompt engineering = high alignment ğŸ’¡

---

## ğŸ’¡ To Reproduce This Project

> Clone this repo and run notebooks in Colab.

### ğŸ” Step-by-Step

1. Upload contents of `data/` and `finetuned/` to your Google Drive
2. Open `Diabetes_LLM_Finetune_llama_3.ipynb` â†’ Fine-tune if needed
3. Open `Diabetes_Lllama3_finetune_and_RAG.ipynb` â†’ Run:

   - PDF â†’ Chunking + FAISS
   - Generate responses (RAG + Non-RAG)
   - Evaluate: BLEU, ROUGE-L, BERTScore, CosSim, t-SNE, Hallucination Count

---

## ğŸ” Privacy and Safety Notes

- We **do not send any prompts to OpenAI or external APIs**
- Fully on-device (Colab A100), LoRA finetuning, private PDF ingestion
- Model not a medical device; for educational/research use only

---

## ğŸ“Œ Future Work

- ğŸ“² Mobile deployment via quantization and ONNX
- ğŸŒ Multilingual support (MedDialog-CN)
- ğŸ§¾ Real-time CGM integration
- ğŸ©º Clinical validation using expert review

---

## ğŸ“š References

See full list in [`docs/CSE_574_Project_Final_Report_Team_31.docx`](docs/CSE_574_Project_Final_Report_Team_31.docx)

---

## ğŸ§‘â€ğŸ”¬ Contributions

| Team Member | Contributions                                                               |
| ----------- | --------------------------------------------------------------------------- |
| **Vaibhav** | Finetuning, RAG Setup, Evaluation, Result Analysis, Dataset Curation        |
| **Rohan**   | Prompting, Dataset Preparation, LoRA Config, Hallucination Detection, t-SNE |
